"""
√âTAPE 2 : Nettoyage et stockage des donn√©es Scopus
"""
import pandas as pd
import sqlite3
import json
import re
from pathlib import Path
from datetime import datetime

class ScopusDataProcessor:
    def __init__(self):
        self.db_path = 'data/processed/scopus_database.db'
        print("üîß Initialisation du processeur de donn√©es Scopus")
        self.setup_database()
    
    def setup_database(self):
        """
        √âTAPE 2.1 : Cr√©ation de la structure de base de donn√©es
        Selon les sp√©cifications du prof : tables relationnelles
        """
        print("üèóÔ∏è Cr√©ation de la structure de base de donn√©es...")
        
        # Cr√©er le dossier s'il n'existe pas
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        
        # Table articles (champs requis par le prof)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scopus_id TEXT UNIQUE NOT NULL,
                title TEXT NOT NULL,
                abstract TEXT,
                cover_date TEXT,
                year INTEGER,
                publication_name TEXT,
                doi TEXT,
                keywords TEXT,
                subject_areas TEXT,
                citation_count INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        print("  ‚úÖ Table 'articles' cr√©√©e")
        
        # Table auteurs (selon sp√©cifications)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS authors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scopus_author_id TEXT,
                preferred_name TEXT NOT NULL,
                orcid TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        print("  ‚úÖ Table 'authors' cr√©√©e")
        
        # Table affiliations (selon sp√©cifications)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS affiliations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scopus_affiliation_id TEXT,
                institution_name TEXT NOT NULL,
                country TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        print("  ‚úÖ Table 'affiliations' cr√©√©e")
        
        # Relations auteur-article (many-to-many)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS article_authors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER NOT NULL,
                author_id INTEGER NOT NULL,
                FOREIGN KEY (article_id) REFERENCES articles (id),
                FOREIGN KEY (author_id) REFERENCES authors (id),
                UNIQUE(article_id, author_id)
            )
        ''')
        print("  ‚úÖ Table 'article_authors' (relations) cr√©√©e")
        
        # Index pour optimisation (comme demand√© par le prof)
        conn.execute('CREATE INDEX IF NOT EXISTS idx_articles_year ON articles(year)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_articles_title ON articles(title)')
        print("  ‚úÖ Index d'optimisation cr√©√©s")
        
        conn.commit()
        conn.close()
        print("‚úÖ Structure de base de donn√©es termin√©e\n")
    
    def clean_text(self, text):
        """
        √âTAPE 2.2 : Nettoyage des caract√®res sp√©ciaux
        Traitement des caract√®res sp√©ciaux et valeurs manquantes (requis par le prof)
        """
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        # Suppression des caract√®res de contr√¥le
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        # Normalisation des espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def extract_year(self, date_str):
        """
        √âTAPE 2.3 : Extraction de l'ann√©e depuis cover_date
        """
        if pd.isna(date_str) or date_str == '':
            return None
        
        # Recherche de l'ann√©e (4 chiffres)
        year_match = re.search(r'(\d{4})', str(date_str))
        if year_match:
            year = int(year_match.group(1))
            # Validation de l'ann√©e
            if 1900 <= year <= 2030:
                return year
        return None
    
    def normalize_authors(self, authors_str):
        """
        √âTAPE 2.4 : Normalisation des auteurs
        Selon sp√©cifications : normalisation des auteurs
        """
        if pd.isna(authors_str) or authors_str == '':
            return []
        
        # S√©paration par virgule, point-virgule ou "and"
        authors = re.split(r'[;,]|\sand\s', str(authors_str))
        
        # Nettoyage de chaque nom
        clean_authors = []
        for author in authors:
            author = self.clean_text(author)
            if len(author) > 1:  # √âviter les initiales seules
                clean_authors.append(author)
        
        return clean_authors
    
    def load_and_clean_data(self, json_file_path):
        """
        √âTAPE 2.5 : Chargement et nettoyage avec Pandas
        Utilisation de Pandas comme demand√© par le prof
        """
        print(f"üìÇ Chargement des donn√©es depuis {json_file_path}")
        
        # Chargement du fichier JSON
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Conversion en DataFrame Pandas
        df = pd.DataFrame(data)
        print(f"üìä {len(df)} articles charg√©s dans Pandas DataFrame")
        
        print("üßπ Nettoyage des donn√©es avec Pandas...")
        
        # √âTAPE 2.5.1 : Suppression des doublons (requis par le prof)
        initial_count = len(df)
        df = df.drop_duplicates(subset=['scopus_id'], keep='first')
        duplicates_removed = initial_count - len(df)
        if duplicates_removed > 0:
            print(f"  üóëÔ∏è {duplicates_removed} doublons supprim√©s")
        else:
            print("  ‚úÖ Aucun doublon d√©tect√©")
        
        # √âTAPE 2.5.2 : Nettoyage des champs textuels
        print("  üßΩ Nettoyage des caract√®res sp√©ciaux...")
        text_fields = ['title', 'publication_name', 'keywords', 'subject_areas']
        for field in text_fields:
            if field in df.columns:
                df[field] = df[field].apply(self.clean_text)
                print(f"    ‚úÖ Champ '{field}' nettoy√©")
        
        # √âTAPE 2.5.3 : Extraction de l'ann√©e
        print("  üìÖ Extraction des ann√©es...")
        df['year'] = df['cover_date'].apply(self.extract_year)
        years_extracted = df['year'].notna().sum()
        print(f"    ‚úÖ {years_extracted} ann√©es extraites")
        
        # √âTAPE 2.5.4 : Traitement des valeurs manquantes
        print("  üîß Traitement des valeurs manquantes...")
        df['abstract'] = df['abstract'].fillna('')
        df['keywords'] = df['keywords'].fillna('')
        df['subject_areas'] = df['subject_areas'].fillna('')
        df['citation_count'] = pd.to_numeric(df['citation_count'], errors='coerce').fillna(0)
        print("    ‚úÖ Valeurs manquantes trait√©es")
        
        print(f"‚úÖ Nettoyage termin√© : {len(df)} articles propres\n")
        return df
    
    def store_articles(self, df):
        """
        √âTAPE 2.6 : Stockage des articles en base relationnelle
        """
        print("üíæ Stockage des articles en base de donn√©es...")
        
        conn = sqlite3.connect(self.db_path)
        articles_stored = 0
        
        try:
            for _, row in df.iterrows():
                # Insertion de chaque article
                cursor = conn.execute('''
                    INSERT OR REPLACE INTO articles 
                    (scopus_id, title, abstract, cover_date, year, publication_name, 
                     doi, keywords, subject_areas, citation_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    row['scopus_id'],
                    row['title'],
                    row.get('abstract', ''),
                    row['cover_date'],
                    row['year'],
                    row['publication_name'],
                    row.get('doi', ''),
                    row['keywords'],
                    row['subject_areas'],
                    int(row['citation_count'])
                ))
                articles_stored += 1
            
            conn.commit()
            print(f"  ‚úÖ {articles_stored} articles stock√©s")
            
        except Exception as e:
            conn.rollback()
            print(f"  ‚ùå Erreur lors du stockage : {e}")
            raise
        finally:
            conn.close()
        
        return articles_stored
    
    def store_authors_and_relations(self, df):
        """
        √âTAPE 2.7 : Stockage des auteurs et cr√©ation des relations
        Relations auteur-article comme demand√© par le prof
        """
        print("üë• Traitement des auteurs et relations...")
        
        conn = sqlite3.connect(self.db_path)
        authors_stored = 0
        relations_created = 0
        
        try:
            for _, row in df.iterrows():
                # R√©cup√©ration de l'ID de l'article
                cursor = conn.execute('SELECT id FROM articles WHERE scopus_id = ?', (row['scopus_id'],))
                article_result = cursor.fetchone()
                if not article_result:
                    continue
                
                article_id = article_result[0]
                
                # Traitement des auteurs
                authors = self.normalize_authors(row.get('authors', ''))
                
                for author_name in authors:
                    if author_name:
                        # Insertion ou r√©cup√©ration de l'auteur
                        conn.execute('''
                            INSERT OR IGNORE INTO authors (preferred_name)
                            VALUES (?)
                        ''', (author_name,))
                        
                        # R√©cup√©ration de l'ID de l'auteur
                        cursor = conn.execute('''
                            SELECT id FROM authors WHERE preferred_name = ?
                        ''', (author_name,))
                        author_id = cursor.fetchone()[0]
                        
                        # Cr√©ation de la relation article-auteur
                        cursor = conn.execute('''
                            INSERT OR IGNORE INTO article_authors (article_id, author_id)
                            VALUES (?, ?)
                        ''', (article_id, author_id))
                        
                        if cursor.rowcount > 0:
                            relations_created += 1
                        
                        authors_stored += 1
            
            conn.commit()
            
            # Comptage des auteurs uniques
            cursor = conn.execute('SELECT COUNT(*) FROM authors')
            unique_authors = cursor.fetchone()[0]
            
            print(f"  ‚úÖ {unique_authors} auteurs uniques stock√©s")
            print(f"  ‚úÖ {relations_created} relations article-auteur cr√©√©es")
            
        except Exception as e:
            conn.rollback()
            print(f"  ‚ùå Erreur lors du traitement des auteurs : {e}")
            raise
        finally:
            conn.close()
        
        return unique_authors, relations_created
    
    def generate_statistics(self):
        """
        √âTAPE 2.8 : G√©n√©ration des statistiques de validation
        """
        print("üìä G√©n√©ration des statistiques...")
        
        conn = sqlite3.connect(self.db_path)
        
        # Statistiques g√©n√©rales
        cursor = conn.execute('SELECT COUNT(*) FROM articles')
        total_articles = cursor.fetchone()[0]
        
        cursor = conn.execute('SELECT COUNT(*) FROM authors')
        total_authors = cursor.fetchone()[0]
        
        cursor = conn.execute('SELECT COUNT(*) FROM article_authors')
        total_relations = cursor.fetchone()[0]
        
        # Articles par ann√©e
        cursor = conn.execute('''
            SELECT year, COUNT(*) 
            FROM articles 
            WHERE year IS NOT NULL 
            GROUP BY year 
            ORDER BY year DESC
        ''')
        by_year = cursor.fetchall()
        
        # √âchantillon d'articles
        cursor = conn.execute('''
            SELECT title, year, publication_name 
            FROM articles 
            LIMIT 3
        ''')
        sample_articles = cursor.fetchall()
        
        conn.close()
        
        # Affichage des statistiques
        print("\nüìà STATISTIQUES FINALES:")
        print(f"  üìö Articles stock√©s: {total_articles}")
        print(f"  üë• Auteurs uniques: {total_authors}")
        print(f"  üîó Relations cr√©√©es: {total_relations}")
        
        if by_year:
            print("  üìÖ Articles par ann√©e:")
            for year, count in by_year:
                print(f"    {year}: {count} articles")
        
        print("\nüìñ √âchantillon d'articles:")
        for i, (title, year, journal) in enumerate(sample_articles, 1):
            print(f"  {i}. {title[:50]}...")
            print(f"     Ann√©e: {year}, Journal: {journal}")
        
        return {
            'total_articles': total_articles,
            'total_authors': total_authors,
            'total_relations': total_relations,
            'by_year': by_year
        }
    
    def process_complete_pipeline(self, json_file_path):
        """
        PIPELINE COMPLET : Ex√©cute toutes les √©tapes de nettoyage et stockage
        """
        print("üöÄ D√âBUT DU PIPELINE DE NETTOYAGE ET STOCKAGE")
        print("=" * 60)
        
        try:
            # √âTAPE 1 : Chargement et nettoyage avec Pandas
            df = self.load_and_clean_data(json_file_path)
            
            # √âTAPE 2 : Stockage des articles
            articles_count = self.store_articles(df)
            
            # √âTAPE 3 : Stockage des auteurs et relations
            authors_count, relations_count = self.store_authors_and_relations(df)
            
            # √âTAPE 4 : G√©n√©ration des statistiques
            stats = self.generate_statistics()
            
            print(f"\n‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!")
            print(f"üìÅ Base de donn√©es cr√©√©e: {self.db_path}")
            print(f"üéì Conforme aux sp√©cifications du professeur")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERREUR DANS LE PIPELINE: {e}")
            return False

def main():
    """
    FONCTION PRINCIPALE : Point d'entr√©e du script
    """
    print("üéì PROJET SCOPUS CHATBOT - √âTAPE 2")
    print("Nettoyage et stockage des donn√©es")
    print("=" * 40)
    
    # Recherche automatique du fichier JSON
    import glob
    json_files = glob.glob('data/raw/*.json')
    
    if not json_files:
        print("‚ùå ERREUR: Aucun fichier JSON trouv√© dans data/raw/")
        print("üìã Actions √† faire:")
        print("  1. V√©rifiez que l'√©tape 1 (extraction) a √©t√© ex√©cut√©e")
        print("  2. V√©rifiez que le fichier JSON existe dans data/raw/")
        return
    
    json_file = json_files[0]
    print(f"üéØ Fichier de donn√©es trouv√©: {json_file}")
    
    # Ex√©cution du pipeline
    processor = ScopusDataProcessor()
    success = processor.process_complete_pipeline(json_file)
    
    if success:
        print("\nüéâ √âTAPE 2 TERMIN√âE AVEC SUCC√àS!")
        print("üöÄ Pr√™t pour l'√©tape 3 : Indexation s√©mantique")
    else:
        print("\n‚ùå √âCHEC DE L'√âTAPE 2")
        print("Consultez les messages d'erreur ci-dessus")

if __name__ == "__main__":
    main()
