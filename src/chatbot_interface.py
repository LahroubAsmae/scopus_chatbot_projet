"""
√âTAPE 4 : Interface Chatbot Streamlit
Interface web pour interroger la base d'articles Scopus
"""
import streamlit as st
import sqlite3
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import json
from datetime import datetime

class ScopusChatbot:
    def __init__(self):
        self.db_path = 'data/processed/scopus_database.db'
        self.model_name = 'all-MiniLM-L6-v2'
        self.model = None
        self.faiss_index = None
        self.article_ids = []
        
        # Configuration de la page Streamlit
        st.set_page_config(
            page_title="üî¨ Scopus Research Chatbot",
            page_icon="üî¨",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        self.setup_chatbot()
    
    @st.cache_resource
    def load_model(_self):
        """Charge le mod√®le Sentence Transformer (avec cache)"""
        return SentenceTransformer(_self.model_name)
    
    @st.cache_resource
    def load_faiss_index(_self):
        """Charge l'index FAISS (avec cache)"""
        try:
            faiss_path = 'data/indexes/scopus_faiss.index'
            metadata_path = 'data/indexes/faiss_metadata.pkl'
            
            if Path(faiss_path).exists() and Path(metadata_path).exists():
                index = faiss.read_index(faiss_path)
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                return index, metadata['article_ids']
            else:
                return None, []
        except Exception as e:
            st.error(f"Erreur lors du chargement de l'index FAISS: {e}")
            return None, []
    
    @st.cache_data
    def load_articles_data(_self):
        """Charge les donn√©es des articles (avec cache)"""
        try:
            conn = sqlite3.connect(_self.db_path)
            query = '''
                SELECT 
                    a.id,
                    a.scopus_id,
                    a.title,
                    a.abstract,
                    a.keywords,
                    a.subject_areas,
                    a.year,
                    a.publication_name,
                    a.doi,
                    a.citation_count
                FROM articles a
                ORDER BY a.year DESC, a.title
            '''
            df = pd.read_sql_query(query, conn)
            conn.close()
            return df
        except Exception as e:
            st.error(f"Erreur lors du chargement des articles: {e}")
            return pd.DataFrame()
    
    def setup_chatbot(self):
        """Initialise le chatbot"""
        with st.spinner("üîß Initialisation du chatbot..."):
            self.model = self.load_model()
            self.faiss_index, self.article_ids = self.load_faiss_index()
            self.articles_df = self.load_articles_data()
        
        if self.faiss_index is None:
            st.error("‚ùå Index FAISS non trouv√©. Ex√©cutez d'abord l'√©tape 3 (indexation s√©mantique)")
            st.stop()
    
    def semantic_search(self, query, k=5):
        """Effectue une recherche s√©mantique"""
        if not query.strip():
            return []
        
        try:
            # Vectorisation de la requ√™te
            query_embedding = self.model.encode([query]).astype('float32')
            faiss.normalize_L2(query_embedding)
            
            # Recherche dans l'index FAISS
            scores, indices = self.faiss_index.search(query_embedding, k=k)
            
            # R√©cup√©ration des r√©sultats
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.article_ids):
                    article_id = self.article_ids[idx]
                    article_row = self.articles_df[self.articles_df['id'] == article_id]
                    if not article_row.empty:
                        article = article_row.iloc[0]
                        results.append({
                            'score': float(score),
                            'article': article.to_dict()
                        })
            
            return results
        except Exception as e:
            st.error(f"Erreur lors de la recherche: {e}")
            return []
    
    def generate_answer(self, query, search_results):
        """G√©n√®re une r√©ponse bas√©e sur les r√©sultats de recherche"""
        if not search_results:
            return "‚ùå Aucun article pertinent trouv√© pour votre question."
        
        # Analyse de la requ√™te pour personnaliser la r√©ponse
        query_lower = query.lower()
        
        # Construction de la r√©ponse
        answer_parts = []
        
        # Introduction contextuelle
        if any(word in query_lower for word in ['artificial intelligence', 'ai', 'intelligence artificielle']):
            answer_parts.append("ü§ñ **Concernant l'Intelligence Artificielle dans votre corpus :**")
        elif any(word in query_lower for word in ['machine learning', 'apprentissage automatique']):
            answer_parts.append("üß† **Concernant l'Apprentissage Automatique dans votre corpus :**")
        elif any(word in query_lower for word in ['endoscopy', 'endoscopie']):
            answer_parts.append("üè• **Concernant l'Endoscopie dans votre corpus :**")
        else:
            answer_parts.append(f"üîç **R√©sultats pour votre recherche '{query}' :**")
        
        # R√©sum√© des r√©sultats
        top_result = search_results[0]
        answer_parts.append(f"\n**Article le plus pertinent** (score: {top_result['score']:.3f}) :")
        answer_parts.append(f"üìÑ *{top_result['article']['title']}*")
        answer_parts.append(f"üìÖ Ann√©e: {top_result['article']['year']}")
        answer_parts.append(f"üìñ Journal: {top_result['article']['publication_name']}")
        
        # Analyse des tendances
        years = [r['article']['year'] for r in search_results if r['article']['year']]
        if years:
            avg_year = sum(years) / len(years)
            answer_parts.append(f"\nüìä **Analyse des r√©sultats :**")
            answer_parts.append(f"‚Ä¢ {len(search_results)} articles pertinents trouv√©s")
            answer_parts.append(f"‚Ä¢ Ann√©e moyenne des publications: {avg_year:.0f}")
            answer_parts.append(f"‚Ä¢ Score de pertinence moyen: {np.mean([r['score'] for r in search_results]):.3f}")
        
        # Suggestions
        answer_parts.append(f"\nüí° **Suggestions :**")
        answer_parts.append("‚Ä¢ Consultez les d√©tails des articles ci-dessous")
        answer_parts.append("‚Ä¢ Utilisez les mots-cl√©s des articles pour affiner votre recherche")
        answer_parts.append("‚Ä¢ Explorez les articles connexes par ann√©e ou journal")
        
        return "\n".join(answer_parts)
    
    def display_article_card(self, article, score=None):
        """Affiche une carte d'article"""
        with st.container():
            col1, col2 = st.columns([3, 1])
            
            with col1:
                # Titre avec lien DOI si disponible
                if article.get('doi'):
                    st.markdown(f"### üìÑ [{article['title']}](https://doi.org/{article['doi']})")
                else:
                    st.markdown(f"### üìÑ {article['title']}")
                
                # Informations de base
                col_info1, col_info2, col_info3 = st.columns(3)
                with col_info1:
                    st.metric("üìÖ Ann√©e", article.get('year', 'N/A'))
                with col_info2:
                    st.metric("üìä Citations", article.get('citation_count', 0))
                with col_info3:
                    if score is not None:
                        st.metric("üéØ Score", f"{score:.3f}")
                
                # Journal
                if article.get('publication_name'):
                    st.markdown(f"**üìñ Journal:** {article['publication_name']}")
                
                # Mots-cl√©s
                if article.get('keywords'):
                    st.markdown(f"**üè∑Ô∏è Mots-cl√©s:** {article['keywords']}")
                
                # R√©sum√© (si disponible)
                if article.get('abstract'):
                    with st.expander("üìù R√©sum√©"):
                        st.write(article['abstract'])
            
            with col2:
                # Informations techniques
                st.markdown("**üîç D√©tails:**")
                st.markdown(f"**ID Scopus:** `{article.get('scopus_id', 'N/A')}`")
                if article.get('subject_areas'):
                    st.markdown(f"**Domaines:** {article['subject_areas']}")
            
            st.divider()
    
    def create_visualizations(self):
        """Cr√©e les visualisations des donn√©es"""
        if self.articles_df.empty:
            return
        
        st.subheader("üìä Analyse de votre corpus")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Distribution par ann√©e
            year_counts = self.articles_df['year'].value_counts().sort_index()
            fig_years = px.bar(
                x=year_counts.index, 
                y=year_counts.values,
                title="üìÖ Distribution des articles par ann√©e",
                labels={'x': 'Ann√©e', 'y': 'Nombre d\'articles'}
            )
            fig_years.update_layout(showlegend=False)
            st.plotly_chart(fig_years, use_container_width=True)
        
        with col2:
            # Distribution des citations
            fig_citations = px.histogram(
                self.articles_df, 
                x='citation_count',
                title="üìà Distribution des citations",
                labels={'citation_count': 'Nombre de citations', 'count': 'Nombre d\'articles'}
            )
            st.plotly_chart(fig_citations, use_container_width=True)
        
        # Top journaux
        if 'publication_name' in self.articles_df.columns:
            journal_counts = self.articles_df['publication_name'].value_counts().head(5)
            if not journal_counts.empty:
                fig_journals = px.pie(
                    values=journal_counts.values,
                    names=journal_counts.index,
                    title="üìñ Top 5 des journaux"
                )
                st.plotly_chart(fig_journals, use_container_width=True)
    
    def run_interface(self):
        """Lance l'interface principale"""
        # En-t√™te
        st.title("üî¨ Scopus Research Chatbot")
        st.markdown("*Interrogez intelligemment votre corpus d'articles scientifiques*")
        
        # Sidebar avec statistiques
        with st.sidebar:
            st.header("üìä Statistiques du corpus")
            
            if not self.articles_df.empty:
                st.metric("üìö Total articles", len(self.articles_df))
                st.metric("üìÖ Ann√©es couvertes", 
                         f"{self.articles_df['year'].min():.0f} - {self.articles_df['year'].max():.0f}")
                st.metric("üìà Citations totales", self.articles_df['citation_count'].sum())
                st.metric("üß† Vecteurs index√©s", len(self.article_ids))
            
            st.divider()
            
            # Exemples de questions
            st.subheader("üí° Exemples de questions")
            example_queries = [
                "Articles sur l'IA m√©dicale",
                "Machine learning pour les mat√©riaux",
                "Endoscopie assist√©e par IA",
                "R√©seaux de neurones",
                "Intelligence artificielle 2026"
            ]
            
            for query in example_queries:
                if st.button(f"üîç {query}", key=f"example_{query}"):
                    st.session_state.query_input = query
        
        # Interface de chat principale
        st.subheader("üí¨ Posez votre question")
        
        # Zone de saisie
        query = st.text_input(
            "Tapez votre question ici:",
            placeholder="Ex: Quels sont les articles sur l'intelligence artificielle en m√©decine ?",
            key="query_input"
        )
        
        # Bouton de recherche
        col1, col2, col3 = st.columns([1, 1, 4])
        with col1:
            search_button = st.button("üîç Rechercher", type="primary")
        with col2:
            clear_button = st.button("üóëÔ∏è Effacer")
        
        if clear_button:
            st.session_state.query_input = ""
            st.rerun()
        
        # Traitement de la recherche
        if search_button and query:
            with st.spinner("üîç Recherche en cours..."):
                # Recherche s√©mantique
                results = self.semantic_search(query, k=5)
                
                if results:
                    # G√©n√©ration de la r√©ponse
                    answer = self.generate_answer(query, results)
                    
                    # Affichage de la r√©ponse
                    st.subheader("ü§ñ R√©ponse du chatbot")
                    st.markdown(answer)
                    
                    # Affichage des articles trouv√©s
                    st.subheader("üìö Articles pertinents")
                    
                    for i, result in enumerate(results, 1):
                        st.markdown(f"#### üìÑ R√©sultat {i}")
                        self.display_article_card(result['article'], result['score'])
                else:
                    st.warning("‚ùå Aucun article pertinent trouv√©. Essayez avec d'autres mots-cl√©s.")
        
        # Onglets pour fonctionnalit√©s suppl√©mentaires
        tab1, tab2, tab3 = st.tabs(["üìä Visualisations", "üìö Tous les articles", "‚ÑπÔ∏è √Ä propos"])
        
        with tab1:
            self.create_visualizations()
        
        with tab2:
            st.subheader("üìö Corpus complet")
            if not self.articles_df.empty:
                # Filtres
                col1, col2 = st.columns(2)
                with col1:
                    year_filter = st.selectbox(
                        "Filtrer par ann√©e:",
                        ["Toutes"] + sorted(self.articles_df['year'].dropna().unique().tolist(), reverse=True)
                    )
                with col2:
                    sort_by = st.selectbox(
                        "Trier par:",
                        ["Ann√©e (desc)", "Ann√©e (asc)", "Citations (desc)", "Titre"]
                    )
                
                # Application des filtres
                filtered_df = self.articles_df.copy()
                if year_filter != "Toutes":
                    filtered_df = filtered_df[filtered_df['year'] == year_filter]
                
                # Tri
                if sort_by == "Ann√©e (desc)":
                    filtered_df = filtered_df.sort_values('year', ascending=False)
                elif sort_by == "Ann√©e (asc)":
                    filtered_df = filtered_df.sort_values('year', ascending=True)
                elif sort_by == "Citations (desc)":
                    filtered_df = filtered_df.sort_values('citation_count', ascending=False)
                elif sort_by == "Titre":
                    filtered_df = filtered_df.sort_values('title')
                
                # Affichage
                for _, article in filtered_df.iterrows():
                    self.display_article_card(article.to_dict())
        
        with tab3:
            st.subheader("‚ÑπÔ∏è √Ä propos de ce chatbot")
            st.markdown("""
            ### üéì Projet Scopus Chatbot
            
            **D√©velopp√© dans le cadre du cours de [Nom du cours]**
            
            #### üîß Technologies utilis√©es:
            - **Extraction de donn√©es**: API Scopus
            - **Nettoyage**: Pandas, SQLite
            - **Indexation s√©mantique**: Sentence Transformers, FAISS
            - **Interface**: Streamlit
            - **Visualisations**: Plotly
            
            #### üìä Corpus analys√©:
            - **Articles**: 10 publications scientifiques
            - **Domaines**: Intelligence Artificielle, Machine Learning
            - **P√©riode**: 2026
            - **Sources**: Base Scopus
            
            #### üöÄ Fonctionnalit√©s:
            - ‚úÖ Recherche s√©mantique intelligente
            - ‚úÖ R√©ponses contextualis√©es
            - ‚úÖ Visualisations interactives
            - ‚úÖ Interface web moderne
            
            ---
            *D√©velopp√© avec ‚ù§Ô∏è pour l'analyse scientifique*
            """)

def main():
    """Fonction principale"""
    try:
        chatbot = ScopusChatbot()
        chatbot.run_interface()
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'initialisation: {e}")
        st.info("üìã V√©rifiez que les √©tapes pr√©c√©dentes ont √©t√© ex√©cut√©es correctement.")

if __name__ == "__main__":
    main()
