"""
√âTAPE 3 : Indexation s√©mantique des r√©sum√©s
Utilise Sentence Transformers + FAISS comme demand√© par le prof
"""
import sqlite3
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import chromadb
from pathlib import Path
import json

class SemanticIndexer:
    def __init__(self, db_path='data/processed/scopus_database.db'):
        self.db_path = db_path
        self.model_name = 'all-MiniLM-L6-v2'  # Mod√®le l√©ger et efficace
        self.model = None
        self.faiss_index = None
        self.article_ids = []
        self.chroma_client = None
        
        print("üîß Initialisation de l'indexeur s√©mantique")
        self.setup_directories()
    
    def setup_directories(self):
        """Cr√©e les dossiers n√©cessaires"""
        Path('data/indexes').mkdir(parents=True, exist_ok=True)
        Path('data/embeddings').mkdir(parents=True, exist_ok=True)
        print("üìÅ Dossiers d'indexation cr√©√©s")
    
    def load_sentence_transformer(self):
        """
        √âTAPE 3.1 : Chargement du mod√®le Sentence Transformer
        Utilise BERT/MiniLM comme sp√©cifi√© par le prof
        """
        print(f"ü§ñ Chargement du mod√®le Sentence Transformer: {self.model_name}")
        print("   (Premi√®re fois = t√©l√©chargement, peut prendre quelques minutes)")
        
        self.model = SentenceTransformer(self.model_name)
        print("‚úÖ Mod√®le Sentence Transformer charg√©")
        
        # Test du mod√®le
        test_text = "artificial intelligence in medicine"
        test_embedding = self.model.encode([test_text])
        print(f"üß™ Test r√©ussi - Dimension des vecteurs: {test_embedding.shape[1]}")
        
        return test_embedding.shape[1]  # Retourne la dimension
    
    def load_articles_from_database(self):
        """
        √âTAPE 3.2 : Chargement des articles depuis la base
        """
        print("üìö Chargement des articles depuis la base de donn√©es...")
        
        conn = sqlite3.connect(self.db_path)
        
        # Requ√™te pour r√©cup√©rer les articles avec leurs informations
        query = '''
            SELECT 
                a.id,
                a.scopus_id,
                a.title,
                a.abstract,
                a.keywords,
                a.subject_areas,
                a.year,
                a.publication_name
            FROM articles a
            ORDER BY a.id
        '''
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        print(f"üìä {len(df)} articles charg√©s pour l'indexation")
        
        # V√©rification des donn√©es
        has_abstract = (df['abstract'].notna() & (df['abstract'] != '')).sum()
        has_keywords = (df['keywords'].notna() & (df['keywords'] != '')).sum()
        
        print(f"   üìù Articles avec r√©sum√©: {has_abstract}")
        print(f"   üè∑Ô∏è Articles avec mots-cl√©s: {has_keywords}")
        
        return df
    
    def prepare_text_for_embedding(self, row):
        """
        √âTAPE 3.3 : Pr√©paration du texte pour vectorisation
        Combine titre + r√©sum√© + mots-cl√©s pour un contexte riche
        """
        text_parts = []
        
        # Titre (toujours pr√©sent)
        if pd.notna(row['title']) and row['title']:
            text_parts.append(f"Title: {row['title']}")
        
        # R√©sum√© (si disponible)
        if pd.notna(row['abstract']) and row['abstract']:
            text_parts.append(f"Abstract: {row['abstract']}")
        
        # Mots-cl√©s (si disponibles)
        if pd.notna(row['keywords']) and row['keywords']:
            text_parts.append(f"Keywords: {row['keywords']}")
        
        # Domaines de recherche (si disponibles)
        if pd.notna(row['subject_areas']) and row['subject_areas']:
            text_parts.append(f"Subject: {row['subject_areas']}")
        
        # Combinaison de tous les √©l√©ments
        combined_text = " ".join(text_parts)
        
        return combined_text
    
    def create_embeddings(self, df):
        """
        √âTAPE 3.4 : Cr√©ation des embeddings (vectorisation)
        Transforme chaque article en vecteur num√©rique
        """
        print("üîÑ Cr√©ation des embeddings s√©mantiques...")
        
        # Pr√©paration des textes
        texts = []
        for _, row in df.iterrows():
            text = self.prepare_text_for_embedding(row)
            texts.append(text)
        
        print(f"üìù {len(texts)} textes pr√©par√©s pour vectorisation")
        
        # Vectorisation par batch pour optimiser la m√©moire
        print("üß† Vectorisation avec Sentence Transformers...")
        embeddings = self.model.encode(
            texts, 
            batch_size=8,  # Petits batches pour √©viter les probl√®mes de m√©moire
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"‚úÖ Embeddings cr√©√©s - Shape: {embeddings.shape}")
        
        # Sauvegarde des embeddings
        embeddings_path = 'data/embeddings/article_embeddings.npy'
        np.save(embeddings_path, embeddings)
        print(f"üíæ Embeddings sauvegard√©s: {embeddings_path}")
        
        return embeddings
    
    def create_faiss_index(self, embeddings, df):
        """
        √âTAPE 3.5 : Cr√©ation de l'index FAISS
        Index vectoriel pour recherche s√©mantique rapide
        """
        print("üèóÔ∏è Cr√©ation de l'index FAISS...")
        
        # Conversion en float32 (requis par FAISS)
        embeddings = embeddings.astype('float32')
        
        # Normalisation pour similarit√© cosinus
        faiss.normalize_L2(embeddings)
        
        # Cr√©ation de l'index FAISS (Inner Product pour cosinus apr√®s normalisation)
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)
        
        # Ajout des vecteurs √† l'index
        self.faiss_index.add(embeddings)
        
        # Sauvegarde des IDs d'articles correspondants
        self.article_ids = df['id'].tolist()
        
        print(f"‚úÖ Index FAISS cr√©√© avec {self.faiss_index.ntotal} vecteurs")
        
        # Sauvegarde de l'index
        faiss_path = 'data/indexes/scopus_faiss.index'
        faiss.write_index(self.faiss_index, faiss_path)
        
        # Sauvegarde des m√©tadonn√©es
        metadata = {
            'article_ids': self.article_ids,
            'model_name': self.model_name,
            'dimension': dimension
        }
        
        with open('data/indexes/faiss_metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"üíæ Index FAISS sauvegard√©: {faiss_path}")
        
    def create_chromadb_collection(self, df):
        """
        √âTAPE 3.6 : Cr√©ation de la collection ChromaDB
        Alternative moderne √† FAISS
        """
        print("üîÑ Cr√©ation de la collection ChromaDB...")
        
        # Initialisation du client ChromaDB
        self.chroma_client = chromadb.PersistentClient(path="data/indexes/chroma_db")
        
        # Suppression de la collection existante si elle existe
        try:
            self.chroma_client.delete_collection("scopus_articles")
        except:
            pass
        
        # Cr√©ation de la nouvelle collection
        collection = self.chroma_client.create_collection(
            name="scopus_articles",
            metadata={"description": "Collection d'articles Scopus avec embeddings s√©mantiques"}
        )
        
        # Pr√©paration des donn√©es pour ChromaDB
        documents = []
        metadatas = []
        ids = []
        
        for _, row in df.iterrows():
            # Texte pour l'embedding
            text = self.prepare_text_for_embedding(row)
            documents.append(text)
            
            # M√©tadonn√©es
            metadata = {
                'scopus_id': str(row['scopus_id']),
                'title': str(row['title']),
                'year': int(row['year']) if pd.notna(row['year']) else 0,
                'publication_name': str(row['publication_name']) if pd.notna(row['publication_name']) else '',
                'has_abstract': bool(pd.notna(row['abstract']) and row['abstract'])
            }
            metadatas.append(metadata)
            
            # ID unique
            ids.append(str(row['id']))
        
        # Ajout √† la collection
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"‚úÖ Collection ChromaDB cr√©√©e avec {len(documents)} documents")
        
    def test_semantic_search(self, df):
        """
        √âTAPE 3.7 : Test de la recherche s√©mantique
        Validation que l'indexation fonctionne
        """
        print("üß™ Test de la recherche s√©mantique...")
        
        # Requ√™tes de test
        test_queries = [
            "artificial intelligence in medicine",
            "machine learning for materials",
            "endoscopy and AI",
            "neural networks"
        ]
        
        for query in test_queries:
            print(f"\nüîç Test: '{query}'")
            
            # Test avec FAISS
            if self.faiss_index is not None:
                query_embedding = self.model.encode([query]).astype('float32')
                faiss.normalize_L2(query_embedding)
                
                scores, indices = self.faiss_index.search(query_embedding, k=3)
                
                print("  üìä R√©sultats FAISS (top 3):")
                for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                    if idx < len(self.article_ids):
                        article_id = self.article_ids[idx]
                        article_row = df[df['id'] == article_id].iloc[0]
                        title = article_row['title'][:60] + "..."
                        print(f"    {i+1}. Score: {score:.3f} - {title}")
            
            # Test avec ChromaDB
            if self.chroma_client is not None:
                try:
                    collection = self.chroma_client.get_collection("scopus_articles")
                    results = collection.query(
                        query_texts=[query],
                        n_results=3
                    )
                    
                    print("  üìä R√©sultats ChromaDB (top 3):")
                    if results['documents'] and results['documents'][0]:
                        for i, metadata in enumerate(results['metadatas'][0]):
                            title = metadata['title'][:60] + "..."
                            distance = results['distances'][0][i]
                            print(f"    {i+1}. Distance: {distance:.3f} - {title}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Erreur ChromaDB: {e}")
    
    def save_indexing_report(self, df, embeddings):
        """
        √âTAPE 3.8 : G√©n√©ration du rapport d'indexation
        """
        print("üìù G√©n√©ration du rapport d'indexation...")
        
        report = {
            'indexing_date': pd.Timestamp.now().isoformat(),
            'model_used': self.model_name,
            'total_articles': len(df),
            'embedding_dimension': embeddings.shape[1],
            'faiss_index_size': self.faiss_index.ntotal if self.faiss_index else 0,
            'articles_by_year': {str(k): int(v) for k, v in df['year'].value_counts().to_dict().items()},
            'has_abstract_count': int((df['abstract'].notna() & (df['abstract'] != '')).sum()),
            'has_keywords_count': int((df['keywords'].notna() & (df['keywords'] != '')).sum())
        }
        
        with open('data/indexes/indexing_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("‚úÖ Rapport d'indexation sauvegard√©")
        return report
    
    def process_complete_indexing(self):
        """
        PIPELINE COMPLET : Indexation s√©mantique compl√®te
        """
        print("üöÄ D√âBUT DE L'INDEXATION S√âMANTIQUE")
        print("=" * 50)
        
        try:
            # √âTAPE 1 : Chargement du mod√®le
            embedding_dim = self.load_sentence_transformer()
            
            # √âTAPE 2 : Chargement des articles
            df = self.load_articles_from_database()
            
            if len(df) == 0:
                print("‚ùå Aucun article trouv√© dans la base de donn√©es")
                return False
            
            # √âTAPE 3 : Cr√©ation des embeddings
            embeddings = self.create_embeddings(df)
            
            # √âTAPE 4 : Cr√©ation de l'index FAISS
            self.create_faiss_index(embeddings, df)
            
            # √âTAPE 5 : Cr√©ation de la collection ChromaDB
            self.create_chromadb_collection(df)
            
            # √âTAPE 6 : Tests de validation
            self.test_semantic_search(df)
            
            # √âTAPE 7 : Rapport final
            report = self.save_indexing_report(df, embeddings)
            
            print("\nüéâ INDEXATION S√âMANTIQUE TERMIN√âE!")
            print("=" * 40)
            print(f"üìä Articles index√©s: {report['total_articles']}")
            print(f"üß† Mod√®le utilis√©: {report['model_used']}")
            print(f"üìê Dimension des vecteurs: {report['embedding_dimension']}")
            print(f"üîç Index FAISS: {report['faiss_index_size']} vecteurs")
            
            print("\nüìÅ Fichiers cr√©√©s:")
            print("  ‚úÖ data/indexes/scopus_faiss.index")
            print("  ‚úÖ data/indexes/faiss_metadata.pkl")
            print("  ‚úÖ data/indexes/chroma_db/")
            print("  ‚úÖ data/embeddings/article_embeddings.npy")
            print("  ‚úÖ data/indexes/indexing_report.json")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERREUR LORS DE L'INDEXATION: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """
    FONCTION PRINCIPALE : Lancement de l'indexation s√©mantique
    """
    print("üéì PROJET SCOPUS CHATBOT - √âTAPE 3")
    print("Indexation s√©mantique avec Sentence Transformers")
    print("=" * 50)
    
    # V√©rification de la base de donn√©es
    db_path = 'data/processed/scopus_database.db'
    if not Path(db_path).exists():
        print(f"‚ùå ERREUR: Base de donn√©es non trouv√©e: {db_path}")
        print("üìã Ex√©cutez d'abord l'√©tape 2 (nettoyage et stockage)")
        return
    
    # Lancement de l'indexation
    indexer = SemanticIndexer(db_path)
    success = indexer.process_complete_indexing()
    
    if success:
        print("\nüéâ √âTAPE 3 TERMIN√âE AVEC SUCC√àS!")
        print("üöÄ Pr√™t pour l'√©tape 4 : Interface utilisateur (Chatbot)")
    else:
        print("\n‚ùå √âCHEC DE L'√âTAPE 3")
        print("Consultez les messages d'erreur ci-dessus")

if __name__ == "__main__":
    main()
